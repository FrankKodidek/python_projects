{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !apt update\n# !apt install ffmpeg libsm6 libxext6 -y","metadata":{"cell_id":"3115c7ea0e40449e9cd02279bd38c2e3","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"e21bc034","execution_start":1648423112930,"execution_millis":3693,"deepnote_cell_type":"code","deepnote_cell_height":437.79998779296875,"execution":{"iopub.status.busy":"2022-04-03T22:54:22.615091Z","iopub.execute_input":"2022-04-03T22:54:22.615442Z","iopub.status.idle":"2022-04-03T22:54:22.621983Z","shell.execute_reply.started":"2022-04-03T22:54:22.615387Z","shell.execute_reply":"2022-04-03T22:54:22.621252Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# !pip install opencv-python\n# !pip install --upgrade pip","metadata":{"cell_id":"2afd62fa42814f2ab010ab071147b726","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"cee1b376","execution_start":1648423116631,"execution_millis":10345,"deepnote_cell_type":"code","deepnote_cell_height":215.60000610351562,"execution":{"iopub.status.busy":"2022-04-03T22:54:22.628613Z","iopub.execute_input":"2022-04-03T22:54:22.629272Z","iopub.status.idle":"2022-04-03T22:54:22.632975Z","shell.execute_reply.started":"2022-04-03T22:54:22.629222Z","shell.execute_reply":"2022-04-03T22:54:22.632169Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nimport time\nimport math\nimport json\nfrom pathlib import Path\nfrom PIL import Image, ImageColor\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom scipy import io\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom types import MethodType\nimport random\nimport six\nimport json\nfrom tqdm import tqdm\nimport itertools\n\nfrom collections import OrderedDict","metadata":{"tags":[],"cell_id":"9bb2c135-a1ea-4ede-83f2-d2a2164b92fc","deepnote_to_be_reexecuted":false,"source_hash":"17d7167","execution_start":1648423126988,"execution_millis":14102,"deepnote_cell_type":"code","deepnote_cell_height":423,"deepnote_output_heights":[328],"execution":{"iopub.status.busy":"2022-04-03T22:54:22.634717Z","iopub.execute_input":"2022-04-03T22:54:22.635434Z","iopub.status.idle":"2022-04-03T22:54:24.517165Z","shell.execute_reply.started":"2022-04-03T22:54:22.635393Z","shell.execute_reply":"2022-04-03T22:54:24.516442Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"IMG_ORD = \"channels_last\"\nMERGE_AXIS = -1\n\npretrained_url ='../input/dataset-map/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#     pretrained_url = open('../input/dataset-map/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n\nclasses_file = np.transpose(np.array(json.load(open('../input/dataset-map/dataset_map/dataset_map/classes.json'))))[2]\nprint(classes_file)\n    \n    \nclass_colors = [ImageColor.getcolor('#' + hexc, \"RGB\") for hexc in classes_file]\nprint(class_colors)","metadata":{"cell_id":"ca2c24f042e94b3a8f48e8267725776f","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"822e072e","execution_start":1648423149727,"execution_millis":26,"deepnote_cell_type":"code","deepnote_cell_height":441,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.519607Z","iopub.execute_input":"2022-04-03T22:54:24.519876Z","iopub.status.idle":"2022-04-03T22:54:24.537890Z","shell.execute_reply.started":"2022-04-03T22:54:24.519840Z","shell.execute_reply":"2022-04-03T22:54:24.537069Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_colored_segmentation_image( seg_arr  , n_classes , colors=class_colors ):\n    output_height = seg_arr.shape[0]\n    output_width = seg_arr.shape[1]\n\n    seg_img = np.zeros((output_height, output_width, 3))\n\n    for c in range(n_classes):\n        seg_img[:, :, 0] += ((seg_arr[:, :] == c)*(colors[c][0])).astype('uint8')\n        seg_img[:, :, 1] += ((seg_arr[:, :] == c)*(colors[c][1])).astype('uint8')\n        seg_img[:, :, 2] += ((seg_arr[:, :] == c)*(colors[c][2])).astype('uint8')\n\n    return seg_img ","metadata":{"cell_id":"ac967200d534459a9f9d47b56c081c94","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"b713f531","execution_start":1648423149768,"execution_millis":0,"deepnote_cell_type":"code","deepnote_cell_height":279,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.539502Z","iopub.execute_input":"2022-04-03T22:54:24.540002Z","iopub.status.idle":"2022-04-03T22:54:24.548616Z","shell.execute_reply.started":"2022-04-03T22:54:24.539961Z","shell.execute_reply":"2022-04-03T22:54:24.547724Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def visualize_segmentation( seg_arr , inp_img=None  , n_classes=None , \n    colors=class_colors , class_names=None , overlay_img=False , show_legends=False , \n    prediction_width=None , prediction_height=None  ):\n    \n\n    if n_classes is None:\n        n_classes = np.max(seg_arr)\n\n    seg_img = get_colored_segmentation_image( seg_arr  , n_classes , colors=colors )\n\n    if not inp_img is None:\n        orininal_h = inp_img.shape[0]\n        orininal_w = inp_img.shape[1]\n        seg_img = cv2.resize(seg_img, (orininal_w, orininal_h))\n\n\n    if (not prediction_height is None) and  (not prediction_width is None):\n        seg_img = cv2.resize(seg_img, (prediction_width, prediction_height ))\n        if not inp_img is None:\n            inp_img = cv2.resize(inp_img, (prediction_width, prediction_height ))\n\n\n    if overlay_img:\n        assert not inp_img is None\n        seg_img = overlay_seg_image( inp_img , seg_img  )\n\n\n    if show_legends:\n        assert not class_names is None\n        legend_img = get_legends(class_names , colors=colors )\n\n        seg_img = concat_lenends( seg_img , legend_img )\n\n\n    return seg_img","metadata":{"cell_id":"de30edd2c70940cf9293e06f35dbf1fb","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"5c01ae07","execution_start":1648423149785,"execution_millis":34,"deepnote_cell_type":"code","deepnote_cell_height":693,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.551898Z","iopub.execute_input":"2022-04-03T22:54:24.552417Z","iopub.status.idle":"2022-04-03T22:54:24.562526Z","shell.execute_reply.started":"2022-04-03T22:54:24.552374Z","shell.execute_reply":"2022-04-03T22:54:24.561627Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def check_input(image_input):\n    if type(image_input) is np.ndarray:\n        img = image_input\n    elif  isinstance(image_input, six.string_types)  :\n        img = cv2.imread(image_input, 1)\n    else:\n        raise DataLoaderError(\"get_segmentation_array: Can't process input type {0}\".format(str(type(image_input))))\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:54:24.565040Z","iopub.execute_input":"2022-04-03T22:54:24.565561Z","iopub.status.idle":"2022-04-03T22:54:24.574517Z","shell.execute_reply.started":"2022-04-03T22:54:24.565520Z","shell.execute_reply":"2022-04-03T22:54:24.573526Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_image_array(image_input, width, height, imgNorm=\"sub_mean\"):\n    \n    img = check_input(image_input)\n\n    if imgNorm == \"sub_and_divide\":\n        img = np.float32(cv2.resize(img, (width, height))) / 127.5 - 1\n    elif imgNorm == \"sub_mean\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img[:, :, 0] -= 103.939\n        img[:, :, 1] -= 116.779\n        img[:, :, 2] -= 123.68\n        img = img[:, :, ::-1]\n    elif imgNorm == \"divide\":\n        img = cv2.resize(img, (width, height))\n        img = img.astype(np.float32)\n        img = img/255.0\n        \n    return img\n\ndef get_segmentation_array(image_input, nClasses, width, height, no_reshape=False):\n\n    seg_labels = np.zeros((height, width, nClasses))\n\n    img = check_input(image_input)\n\n    img = cv2.resize(img, (width, height), interpolation=cv2.INTER_NEAREST)\n    img = img[:, :, 0]\n\n    for c in range(nClasses):\n        seg_labels[:, :, c] = (img == c).astype(int)\n\n    if not no_reshape:\n        seg_labels = np.reshape(seg_labels, (width*height, nClasses))\n\n    return seg_labels","metadata":{"cell_id":"64388bd575f3404b969b2a7315a6d0f0","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"1e739fee","execution_start":1648423149819,"execution_millis":1,"deepnote_cell_type":"code","deepnote_cell_height":1557,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.576954Z","iopub.execute_input":"2022-04-03T22:54:24.577678Z","iopub.status.idle":"2022-04-03T22:54:24.590879Z","shell.execute_reply.started":"2022-04-03T22:54:24.577635Z","shell.execute_reply":"2022-04-03T22:54:24.590069Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def image_segmentation_generator(images_path, segs_path, batch_size,\n                                 n_classes, input_height, input_width,\n                                 output_height, output_width):\n\n    img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n    random.shuffle(img_seg_pairs)\n    zipped = itertools.cycle(img_seg_pairs)\n\n    while True:\n        X = []\n        Y = []\n        for _ in range(batch_size):\n            im, seg = next(zipped)\n\n            im = cv2.imread(im, 1)\n            seg = cv2.imread(seg, 1)\n\n            X.append(get_image_array(im, input_width, input_height))\n            Y.append(get_segmentation_array(seg, n_classes, output_width, output_height))\n\n        yield np.array(X), np.array(Y)","metadata":{"cell_id":"309184393bca4b7db8cab282ecc5d47d","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"34c8058e","execution_start":1648423149820,"execution_millis":6,"deepnote_cell_type":"code","deepnote_cell_height":549,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.594095Z","iopub.execute_input":"2022-04-03T22:54:24.594556Z","iopub.status.idle":"2022-04-03T22:54:24.605119Z","shell.execute_reply.started":"2022-04-03T22:54:24.594523Z","shell.execute_reply":"2022-04-03T22:54:24.604223Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def get_pairs_from_paths(images_path, segs_path, ignore_non_matching=False):\n    \n    image_files = []\n    segmentation_files = {}\n\n    for dir_entry in os.listdir(images_path):\n        if os.path.isfile(os.path.join(images_path, dir_entry)):\n            file_name, file_extension = os.path.splitext(dir_entry)\n            \n            image_files.append((file_name, file_extension, os.path.join(images_path, dir_entry)))\n\n    for dir_entry in os.listdir(segs_path):\n        if os.path.isfile(os.path.join(segs_path, dir_entry)):\n            file_name, file_extension = os.path.splitext(dir_entry)\n            \n            segmentation_files[file_name] = (file_extension, os.path.join(segs_path, dir_entry))\n\n    return_value = []\n    \n    for image_file, _, image_full_path in image_files:\n        if image_file in segmentation_files:\n            return_value.append((image_full_path, segmentation_files[image_file][1]))\n        elif ignore_non_matching:\n            continue\n\n    return return_value","metadata":{"cell_id":"70bf52bab1de4e1ca80012ddcf7900df","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"1fb05860","execution_start":1648423149872,"execution_millis":1,"deepnote_cell_type":"code","deepnote_cell_height":729,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.606788Z","iopub.execute_input":"2022-04-03T22:54:24.607699Z","iopub.status.idle":"2022-04-03T22:54:24.618253Z","shell.execute_reply.started":"2022-04-03T22:54:24.607657Z","shell.execute_reply":"2022-04-03T22:54:24.617459Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def verify_segmentation_dataset(images_path, segs_path, n_classes):\n    try:\n        img_seg_pairs = get_pairs_from_paths(images_path, segs_path)\n\n        return_value = True\n        for im_fn, seg_fn in tqdm(img_seg_pairs):\n            img = cv2.imread(im_fn)\n            seg = cv2.imread(seg_fn)\n            \n            if not img.shape == seg.shape:\n                return_value = False\n            else:\n                max_pixel_value = np.max(seg[:, :, 0])\n                if max_pixel_value >= n_classes:\n                    return_value = False\n                    \n        if return_value:\n            print(\"Dataset verified!\")\n        else:\n            print(\"Dataset not verified!\")\n        return return_value\n    except Exception as e:\n        print(str(e))\n        return False","metadata":{"cell_id":"3f50047a4a0d4c8f86d9c22dbb6d9c38","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"1f1982","execution_start":1648423149873,"execution_millis":1,"deepnote_cell_type":"code","deepnote_cell_height":639,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.619756Z","iopub.execute_input":"2022-04-03T22:54:24.620363Z","iopub.status.idle":"2022-04-03T22:54:24.630626Z","shell.execute_reply.started":"2022-04-03T22:54:24.620308Z","shell.execute_reply":"2022-04-03T22:54:24.629883Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def evaluate(model=None ,inp_images=None ,annotations=None, inp_images_dir=None, annotations_dir=None, checkpoints_path=None):\n        \n    if inp_images is None:\n        paths = get_pairs_from_paths(inp_images_dir , annotations_dir)\n        paths = list(zip(*paths))\n        inp_images = list(paths[0])\n        annotations = list(paths[1])\n        \n    tp = np.zeros(model.n_classes)\n    fp = np.zeros(model.n_classes)\n    fn = np.zeros(model.n_classes)\n    n_pixels = np.zeros(model.n_classes)\n    \n    for inp, ann in tqdm(zip(inp_images, annotations)):\n        pr = predict(model , inp )\n        gt = get_segmentation_array(ann, model.n_classes, model.output_width, model.output_height, no_reshape=True)\n        gt = gt.argmax(-1)\n        pr = pr.flatten()\n        gt = gt.flatten()\n                \n        for cl_i in range(model.n_classes):\n            tp[ cl_i ] += np.sum( (pr == cl_i) * (gt == cl_i) )\n            fp[ cl_i ] += np.sum( (pr == cl_i) * ((gt != cl_i)) )\n            fn[ cl_i ] += np.sum( (pr != cl_i) * ((gt == cl_i)) )\n            n_pixels[ cl_i ] += np.sum( gt == cl_i  )\n            \n    cl_wise_score = tp / (tp + fp + fn + 0.000000000001)\n    n_pixels_norm = n_pixels /  np.sum(n_pixels)\n    frequency_weighted_IU = np.sum(cl_wise_score * n_pixels_norm)\n    mean_IU = np.mean(cl_wise_score)\n    \n    return {\"frequency_weighted_IU\":frequency_weighted_IU , \"mean_IU\":mean_IU , \"class_wise_IU\":cl_wise_score }","metadata":{"cell_id":"463024ba9ec64def9a1869ec4fbf28bf","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"6b8b141c","execution_start":1648423149914,"execution_millis":0,"deepnote_cell_type":"code","deepnote_cell_height":819,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.633909Z","iopub.execute_input":"2022-04-03T22:54:24.634125Z","iopub.status.idle":"2022-04-03T22:54:24.649129Z","shell.execute_reply.started":"2022-04-03T22:54:24.634101Z","shell.execute_reply":"2022-04-03T22:54:24.648194Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def predict(model=None, inp=None, out_fname=None, checkpoints_path=None,overlay_img=False ,\n    class_names=None, show_legends=False, colors=class_colors, prediction_width=None, prediction_height=None):\n    \n    inp = cv2.imread(inp)\n    \n    orininal_h, orininal_w, _ = inp.shape\n\n    output_width = model.output_width\n    output_height = model.output_height\n    input_width = model.input_width\n    input_height = model.input_height\n    n_classes = model.n_classes\n\n    x = get_image_array(inp, input_width, input_height)\n    pr = model.predict(np.array([x]))[0]\n    pr = pr.reshape((output_height, output_width, n_classes)).argmax(axis=2)\n\n    seg_img = visualize_segmentation(pr, inp, n_classes=n_classes, colors=colors, overlay_img=overlay_img,\n                                     show_legends=show_legends, class_names=class_names,\n                                     prediction_width=prediction_width, prediction_height=prediction_height)\n\n    if out_fname is not None:\n        cv2.imwrite(out_fname, seg_img)\n\n    return pr","metadata":{"cell_id":"49850ff8b72b4cd38de7a8d674569b02","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"6f2ca40b","execution_start":1648423149925,"execution_millis":8,"deepnote_cell_type":"code","deepnote_cell_height":675,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.653872Z","iopub.execute_input":"2022-04-03T22:54:24.654456Z","iopub.status.idle":"2022-04-03T22:54:24.665597Z","shell.execute_reply.started":"2022-04-03T22:54:24.654413Z","shell.execute_reply":"2022-04-03T22:54:24.664654Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def train(model,\n          train_images,\n          train_annotations,\n          input_height=None,\n          input_width=None,\n          n_classes=None,\n          verify_dataset=True,\n          checkpoints_path=None,\n          epochs=5,\n          batch_size=2,\n          validate=False,\n          val_images=None,\n          val_annotations=None,\n          val_batch_size=2,\n          auto_resume_checkpoint=False,\n          load_weights=None,\n          steps_per_epoch=200,\n          val_steps_per_epoch=100,\n          gen_use_multiprocessing=False,\n          ignore_zero_class=False , \n          optimizer_name='adadelta' , do_augment=False , augmentation_name=\"aug_all\"\n          ):\n\n    \n    \n    \n    if isinstance(model, six.string_types):\n        if (input_height is not None) and (input_width is not None):\n            model = model_from_name[model](\n                n_classes, input_height=input_height, input_width=input_width)\n        else:\n            model = model_from_name[model](n_classes)\n\n    n_classes = model.n_classes\n    input_height = model.input_height\n    input_width = model.input_width\n    output_height = model.output_height\n    output_width = model.output_width\n\n    if optimizer_name is not None:\n\n        if ignore_zero_class:\n            loss_k = masked_categorical_crossentropy\n        else:\n            loss_k = 'categorical_crossentropy'\n\n        model.compile(loss= loss_k, optimizer=optimizer_name, metrics=['accuracy'])\n\n    if load_weights is not None and len(load_weights) > 0:\n        print(\"Loading weights from \", load_weights)\n        model.load_weights(load_weights)\n\n    if verify_dataset:\n        print(\"Verifying training dataset\")\n        verified = verify_segmentation_dataset(train_images, train_annotations, n_classes)\n        assert verified\n\n    train_gen = image_segmentation_generator(train_images, train_annotations, batch_size, n_classes,\n                                             input_height, input_width, output_height, output_width)\n\n    for ep in range(epochs):\n        print(\"Starting Epoch \", ep)\n        model.fit_generator(train_gen, steps_per_epoch, epochs=1, use_multiprocessing=True)\n        print(\"Finished Epoch\", ep)","metadata":{"cell_id":"03c1cabae3e84a179f99ec93f5391e45","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"f7574d9b","execution_start":1648423149955,"execution_millis":3,"deepnote_cell_type":"code","deepnote_cell_height":2097,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.667393Z","iopub.execute_input":"2022-04-03T22:54:24.668088Z","iopub.status.idle":"2022-04-03T22:54:24.682368Z","shell.execute_reply.started":"2022-04-03T22:54:24.668045Z","shell.execute_reply":"2022-04-03T22:54:24.681593Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_segmentation_model(input, output):\n\n    img_input = input\n    o = output\n\n    o_shape = Model(img_input, o).output_shape\n    i_shape = Model(img_input, o).input_shape\n\n    output_height = o_shape[1]\n    output_width = o_shape[2]\n    input_height = i_shape[1]\n    input_width = i_shape[2]\n    n_classes = o_shape[3]\n    o = (Reshape((output_height*output_width, -1)))(o)\n\n    o = (Activation('softmax'))(o)\n#     model = tf.keras.Sequential(img_input, o)\n#     model = tf.keras.Sequential()\n    model = Model(img_input, o)\n    model.output_width = output_width\n    model.output_height = output_height\n    model.n_classes = n_classes\n    model.input_height = input_height\n    model.input_width = input_width\n    model.model_name = \"\"\n\n    model.train = MethodType(train, model)\n    model.predict_segmentation = MethodType(predict, model)\n    model.evaluate_segmentation = MethodType(evaluate, model)\n\n    return model","metadata":{"cell_id":"2bf21bfe227845688554f47b1b223f49","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"d39ad71a","execution_start":1648423149999,"execution_millis":0,"deepnote_cell_type":"code","deepnote_cell_height":765,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.684012Z","iopub.execute_input":"2022-04-03T22:54:24.684387Z","iopub.status.idle":"2022-04-03T22:54:24.695013Z","shell.execute_reply.started":"2022-04-03T22:54:24.684346Z","shell.execute_reply":"2022-04-03T22:54:24.694137Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_vgg_encoder(input_height,  input_width, pretrained='imagenet'):\n\n    assert input_height % 32 == 0\n    assert input_width % 32 == 0\n\n    img_input = Input(shape=(input_height, input_width, 3))\n    \n# Block 1\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv1', data_format=IMG_ORD)(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv2', data_format=IMG_ORD)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool',\n                     data_format=IMG_ORD)(x)\n    f1 = x\n# Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv1', data_format=IMG_ORD)(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv2', data_format=IMG_ORD)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool',\n                     data_format=IMG_ORD)(x)\n    f2 = x\n\n# Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv1', data_format=IMG_ORD)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv2', data_format=IMG_ORD)(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv3', data_format=IMG_ORD)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool',\n                     data_format=IMG_ORD)(x)\n    f3 = x\n\n# Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv1', data_format=IMG_ORD)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv2', data_format=IMG_ORD)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv3', data_format=IMG_ORD)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool',\n                     data_format=IMG_ORD)(x)\n    f4 = x\n\n# Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv1', data_format=IMG_ORD)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv2', data_format=IMG_ORD)(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv3', data_format=IMG_ORD)(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool',\n                     data_format=IMG_ORD)(x)\n    f5 = x\n\n    if pretrained == 'imagenet':\n        VGG_Weights_path = pretrained_url\n        \n        Model(img_input, x).load_weights(VGG_Weights_path)\n\n    return img_input, [f1, f2, f3, f4, f5]","metadata":{"cell_id":"161eda6f691f44f2b0def75cdbb187ee","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"f9b25712","execution_start":1648423150000,"execution_millis":1,"deepnote_cell_type":"code","deepnote_cell_height":1215,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.696600Z","iopub.execute_input":"2022-04-03T22:54:24.697274Z","iopub.status.idle":"2022-04-03T22:54:24.717856Z","shell.execute_reply.started":"2022-04-03T22:54:24.697221Z","shell.execute_reply":"2022-04-03T22:54:24.716757Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def _unet(n_classes, encoder, l1_skip_conn=True, input_height=200, input_width=200):\n\n    img_input, [f1, f2, f3, f4, _] = encoder(input_height=input_height, input_width=input_width)\n\n#     o = f4\n\n#     o = (ZeroPadding2D((1, 1), data_format=IMG_ORD))(o)\n#     o = (Conv2D(512, (3, 3), padding='valid', data_format=IMG_ORD))(o)\n#     o = (BatchNormalization())(o)\n\n#     o = (UpSampling2D((2, 2), data_format=IMG_ORD))(o)\n    \n#     o = (concatenate([o, f3], axis=MERGE_AXIS))\n    o = f3\n    o = (ZeroPadding2D((1, 1), data_format=IMG_ORD))(o)\n    o = (Conv2D(256, (3, 3), padding='valid', data_format=IMG_ORD))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMG_ORD))(o)\n    o = (concatenate([o, f2], axis=MERGE_AXIS))\n    o = (ZeroPadding2D((1, 1), data_format=IMG_ORD))(o)\n    o = (Conv2D(128, (3, 3), padding='valid', data_format=IMG_ORD))(o)\n    o = (BatchNormalization())(o)\n\n    o = (UpSampling2D((2, 2), data_format=IMG_ORD))(o)\n\n    if l1_skip_conn:\n        o = (concatenate([o, f1], axis=MERGE_AXIS))\n\n    o = (ZeroPadding2D((1, 1), data_format=IMG_ORD))(o)\n    o = (Conv2D(64, (3, 3), padding='valid', data_format=IMG_ORD))(o)\n    o = (BatchNormalization())(o)\n\n    o = Conv2D(n_classes, (3, 3), padding='same',data_format=IMG_ORD)(o)\n\n    model = get_segmentation_model(img_input, o)\n\n    return model","metadata":{"cell_id":"4fc85fda09ac4ac0896157aa6d22a608","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"28f544e9","execution_start":1648423150005,"execution_millis":59,"deepnote_cell_type":"code","deepnote_cell_height":765,"execution":{"iopub.status.busy":"2022-04-03T22:54:24.719318Z","iopub.execute_input":"2022-04-03T22:54:24.720107Z","iopub.status.idle":"2022-04-03T22:54:24.733937Z","shell.execute_reply.started":"2022-04-03T22:54:24.720064Z","shell.execute_reply":"2022-04-03T22:54:24.732989Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n# for device in gpu_devices:\n#     tf.config.experimental.set_memory_growth(device, True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:54:24.735280Z","iopub.execute_input":"2022-04-03T22:54:24.736137Z","iopub.status.idle":"2022-04-03T22:54:24.743926Z","shell.execute_reply.started":"2022-04-03T22:54:24.736098Z","shell.execute_reply":"2022-04-03T22:54:24.743107Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:54:24.745394Z","iopub.execute_input":"2022-04-03T22:54:24.745789Z","iopub.status.idle":"2022-04-03T22:54:24.752971Z","shell.execute_reply.started":"2022-04-03T22:54:24.745749Z","shell.execute_reply":"2022-04-03T22:54:24.752165Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n# print(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T22:54:24.755696Z","iopub.execute_input":"2022-04-03T22:54:24.756264Z","iopub.status.idle":"2022-04-03T22:54:24.760314Z","shell.execute_reply.started":"2022-04-03T22:54:24.756223Z","shell.execute_reply":"2022-04-03T22:54:24.759368Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def vgg_unet(n_classes, input_height, input_width, encoder_level=3):\n\n    model = _unet(n_classes, get_vgg_encoder,input_height=input_height, input_width=input_width)\n    model.model_name = \"vgg_unet\"\n    return model\n\nn_classes = len(class_colors)\n\nmodel = vgg_unet(n_classes=n_classes,  input_height=608, input_width=800)\nmodel_from_name = {}\nmodel_from_name[\"vgg_unet\"] = vgg_unet","metadata":{"cell_id":"bd91d43652f341399bcc674ae7ce6e74","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"aaa7165","execution_start":1648423209060,"execution_millis":841,"deepnote_cell_type":"code","deepnote_cell_height":261,"deepnote_output_heights":[611],"execution":{"iopub.status.busy":"2022-04-03T22:54:24.761800Z","iopub.execute_input":"2022-04-03T22:54:24.762423Z","iopub.status.idle":"2022-04-03T22:54:27.075153Z","shell.execute_reply.started":"2022-04-03T22:54:24.762386Z","shell.execute_reply":"2022-04-03T22:54:27.074442Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"epochs = 1","metadata":{"cell_id":"de124210fd2a4db99ae34d9152a2fc5a","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"a3466d41","execution_start":1648423212538,"execution_millis":2,"deepnote_cell_type":"code","deepnote_cell_height":81,"execution":{"iopub.status.busy":"2022-04-03T22:54:27.076268Z","iopub.execute_input":"2022-04-03T22:54:27.076521Z","iopub.status.idle":"2022-04-03T22:54:27.083035Z","shell.execute_reply.started":"2022-04-03T22:54:27.076487Z","shell.execute_reply":"2022-04-03T22:54:27.082226Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model.train( \n    train_images =  \"../input/dataset-map/dataset_map/dataset_map/rgb\",\n    train_annotations = \"../input/dataset-map/dataset_map/dataset_map/label\",\n    epochs=epochs#, batch_size=BATCH_SIZE\n)","metadata":{"cell_id":"b7df07c1f99340578f9008e0c624191b","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"9b52e254","execution_start":1648423213939,"execution_millis":539,"deepnote_cell_type":"code","deepnote_cell_height":804.7333374023438,"deepnote_output_heights":[null,40],"execution":{"iopub.status.busy":"2022-04-03T22:54:27.084450Z","iopub.execute_input":"2022-04-03T22:54:27.084764Z","iopub.status.idle":"2022-04-03T22:57:54.783176Z","shell.execute_reply.started":"2022-04-03T22:54:27.084728Z","shell.execute_reply":"2022-04-03T22:57:54.782107Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\ninput_image = \"../input/dataset-map/dataset_map/dataset_map/rgb/IMG_0197.tif\"\nout = model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-002.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"../input/dataset-map/dataset_map/dataset_map/label/IMG_0197.tif\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-002.png')\naxs[2].grid(False)\n\n\ndone = time.time()\nelapsed = done - start","metadata":{"cell_id":"1c2560ed43004066ba29164528286fbb","tags":[],"deepnote_to_be_reexecuted":true,"source_hash":"6eee9bf6","deepnote_cell_type":"code","deepnote_cell_height":549,"execution":{"iopub.status.busy":"2022-04-03T22:57:54.786043Z","iopub.execute_input":"2022-04-03T22:57:54.786801Z","iopub.status.idle":"2022-04-03T22:58:01.177743Z","shell.execute_reply.started":"2022-04-03T22:57:54.786770Z","shell.execute_reply":"2022-04-03T22:58:01.175465Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d0ebdbe2-6239-480a-91f2-f925ec1458cf' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}]}